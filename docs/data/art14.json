{
  "article": "14",
  "sheet": "GQs_Art.14",
  "requirement": "Art. 14: Human Oversight\n\nNote: human oversight does not refer to the technical monitoring of the AI system (which occur periodically) but to the active and meaningful involvement of human actors during each individual prediction\n\nClarifications about Human Oversight [Recital 73]\n\nHigh-risk AI systems should be designed and developed in such a way that natural persons can oversee their functioning, ensure that they are used as intended and that their impacts are addressed over the system’s lifecycle. To that end, appropriate human oversight measures should be identified by the provider of the system before its placing on the market or putting into service. In particular, where appropriate, such measures should guarantee that the system is subject to in-built operational constraints that cannot be overridden by the system itself and is responsive to the human operator, and that the natural persons to whom human oversight has been assigned have the necessary competence, training and authority to carry out that role. It is also essential, as appropriate, to ensure that high-risk AI systems include mechanisms to guide and inform a natural person to whom human oversight has been assigned to make informed decisions if, when and how to intervene in order to avoid negative consequences or risks, or stop the system if it does not perform as intended. Considering the significant consequences for persons in the case of an incorrect match by certain biometric identification systems, it is appropriate to provide for an enhanced human oversight requirement for those systems so that no action or decision may be taken by the deployer on the basis of the identification resulting from the system unless this has been separately verified and confirmed by at least two natural persons. Those persons could be from one or more entities and include the person operating or using the system. This requirement should not pose unnecessary burden or delays and it could be sufficient that the separate verifications by the different persons are automatically recorded in the logs generated by the system. Given the specificities of the areas of law enforcement, migration, border control and asylum, this requirement should not apply where Union or national law considers the application of that requirement to be disproportionate.\n\nTrasversal Questions",
  "rows": [
    {
      "Compliance checklist": "●       1. The high risk AI systems is designed and developed to enable effective oversight by natural persons during their operational use.",
      "Guiding Questions": "",
      "In-Context": "",
      "_children": [
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.1.1     Is the high-risk AI system designed to keep the human in control during the process of individual decision-making?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.1.2     Who is the human operator responsible for oversight? Is it the developer, the recruiter, or another figure?",
          "In-Context": "X"
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.1.3     In which steps of the pipeline will the human oversight take place (e.g., after each critical step, at the end of the pipeline etc.)?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.1.4     What kind of information will the human overseer be provided to take the final decision in each individual case?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.1.5     How should the interaction between the system and the human operator take place? Has an appropriate interface been designed and developed?",
          "In-Context": ""
        }
      ]
    },
    {
      "Compliance checklist": "●     2. Human oversight measures are such to prevent or minimise the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse",
      "Guiding Questions": "",
      "In-Context": "",
      "_children": [
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.2.1     Given the potential risks to the health, safety or fundamental rights of users or affected individuals that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse as identified according to Art. 9, which human oversight measures are in place to prevent and minimize such risks? Justify your choice mapping each identified risk (or foreseeable misure) to the appropriate human oversight measure.",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.2.2     Are human overseers trained to recognize risks and adopt the appropriate human oversight measures to prevent/minimise them?",
          "In-Context": ""
        }
      ]
    },
    {
      "Compliance checklist": "●     3. Human oversight measures commensurate with the risks, level of autonomy and context of use of the high-risk AI system are ensured through measures  \n(a) identified and built by the provider before it is placed on the market or put into service  \n(b) identified by the provider before putting it on the market and are appropriate to be implemented by the deployer\n[At least one of the following should apply]",
      "Guiding Questions": "",
      "In-Context": "",
      "_children": [
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.3.1     Before placing the high-risk AI system on the market or putting it into service, has the provider identified human oversight measures which are commensurate with the risks, level of autonomy, intended used, and context of use of the high-risk AI system?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.3.2     Have the identified oversight measures been designed into the system (e.g., through actions requiring deployer intervention) by the provider before it is placed on the market or put into service? If yes, how? Document and justify them.",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.3.3 Are the human oversight measures not implemented by the provider (if any) appropriate to be implemented by the deployer? Document and justify your methodology.",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.3.4 Did the provider give instructions to the deployer on how to appropriately implement said human oversight measures? Specify the instructions.",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.3.5 Is there a continuous feedback between provider and deployer on how to appropriately implement human oversight measures as new risk emerge?",
          "In-Context": ""
        }
      ]
    },
    {
      "Compliance checklist": "●    4. For the purpose of implementing paragraphs 1, 2 and 3 of Art. 14, the high-risk AI system shall be provided to the deployer in such a way that natural persons to whom human oversight is assigned are enabled, as appropriate and proportionate:",
      "Guiding Questions": "",
      "In-Context": "",
      "_children": []
    },
    {
      "Compliance checklist": "●     4(a) to properly understand its capabilities and limitations and duly monitor its operation, including in view of detecting and addressing anomalies, dysfunctions and unexpected performance",
      "Guiding Questions": "",
      "In-Context": "",
      "_children": [
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.a.1     Have human overseers received documentation or training about how the system works and its intended purposes, in particular on how to recognize when the input data (e.g., belonging to the target population) and context can be appropriately processed by the system?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.a.2     What is the AI literacy necessary for human overseers to actively and knowledgeably participate in the decision-making and monitoring processes?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.a.3     What specific events of anomalies, dysfunctions and unexpected performance should be reported to the human overseer?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.a.4 How do the human overseers receive information about anomalies, dysfunctions and unexpected performance of the system?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.a.5     In case of failed tests (e.g. anomalies), which interventions should be put in place to properly handle the information thorugh the AI system pipeline (i.e. from data processing to CV ranking)?",
          "In-Context": "X"
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.a.6     Which is the frequency of the monitoring procedure and how does it lead to accurate and timely detection of anomalies, dysfunctions and unexpected performance?",
          "In-Context": ""
        }
      ]
    },
    {
      "Compliance checklist": "●     4(b) to remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system (automation bias), in particular for high-risk AI systems used to provide information or recommendations for decisions to be taken by natural persons",
      "Guiding Questions": "",
      "In-Context": "",
      "_children": [
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.b.1 Which choices in the architecture of the AI system did you put in place to limit the risk of automation bias (e.g. explainable AI, learning to defer, frictional AI)?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.b.2    Are human-AI interaction measures adopted to avoid over-reliance (e.g., progressive disclosure, provision of a set of alternative system outputs rather than single predictions)?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.b.3    Which information will the overseer be provided to prevent automation bias (e.g. prediction, confidence of the model, explanations)?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "● 14.4.b.4 Is the user interface designed and implemented to ensure effective oversight and limit the risk of automation bias?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.b.5    Is human oversight on each case performed by multiple individuals? If so, how do you handle contrastive evaluations?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.b.6    Are the human overseer aware of the possible risks for affected individuals that may derive from automation bias, such as interviewing candidates based on synthetically generated scores without verifying the assessment metrics, including those related to predictive robustness, illegal discrimination and consistency of explanations?",
          "In-Context": "x"
        }
      ]
    },
    {
      "Compliance checklist": "●     4(c) to correctly interpret its output taking into account, for example, the interpretation tools and methods available;",
      "Guiding Questions": "",
      "In-Context": "",
      "_children": [
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.c.1    What information should be provided to the human overseers to enable them to properly interpret the output of the AI system? Does it include any interpretation tools (e.g., feature importance, counterfactuals, examples and counterexamples…)?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.c.2    How are human overseers trained to achieve an appropriate level of AI literacy to correctly interpret the system's output, including the explanations given by the interpretability tools?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "● 14.4.c.3 Were the explanations generated using generative AI?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "○    If so, have you considered that, as potentially unreliable outputs, they may constitute a source of systemic risk (3rd draft of the Code of Practice, Safety and Security, Appendix 1.4)? How do you minimise this risk? Document and justify.",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.c.4    What explainability (XAI) alternatives can be adopted?",
          "In-Context": ""
        }
      ]
    },
    {
      "Compliance checklist": "●   4(d) to decide, in any particular situation, not to use the high-risk AI system or to otherwise disregard, override or reverse the output of the high-risk AI system;",
      "Guiding Questions": "",
      "In-Context": "",
      "_children": [
        {
          "Compliance checklist": "",
          "Guiding Questions": "● 14.4.d.1 In which particular situations do you expect the high-risk AI system to be reasonably not used?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "● 14.4.d.2  Is the AI system embedded with algorithmic mechanisms (e.g., learning to defer) that enable it to automatically recognise, interrupt its execution and defer the decision-making task to humans (for instance, when the confidence score is low, the input data is not consistent with the target population, etc.)?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "● 14.4.d.3 In which particular situations do you expect the output of the high-risk AI system to be reasonably disregarded, overridden or reversed?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "● 14.4.d.4 In said particular situations, how will the human overseer be alerted?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "● 14.4.d.5 How do you design the system in a way that disregarding, overriding and reversing the output can happen in a timely fashion after receiving the alerts?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "● 14.4.d.6 Can human overseers disregard, override and reverse the output of the high-risk AI system, also beyond predefined exceptional cases (e.g., by manually correcting parsing results or adjusting the ranking of shortlisted candidates)?",
          "In-Context": "X"
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "● 14.4.d.7 How are events of disregarding, overriding and reversing of the output of the high-risk AI system traced?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "● 14.4.d.8 Is there a feedback mechanism to integrate in the AI system the evaluation of the human overseer in case it differs from the machine prediction?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "● 14.4.d.9  Is the AI system embedded with algorithmic interventions that enable it to automatically recognise and stop its execution when its level of uncertainty or risk is too high to provide a prediction on critical cases?",
          "In-Context": ""
        }
      ]
    },
    {
      "Compliance checklist": "●   4(e) to intervene in the operation of the high-risk AI system or interrupt the system through a ‘stop’ button or a similar procedure that allows the system to come to a halt in a safe state",
      "Guiding Questions": "",
      "In-Context": "",
      "_children": [
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.e.1   How do you design the procedure to interrupt the AI system (i.e. \"stop button\")? Describe the procedure.",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.e.2    Whenever you recognise the need to stop the AI system operations, how do you handle previous instances processed through the malfunctioning system?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "●14.4.e.3    Are monitoring strategies put in place by human overseers to keep them aware of possible malfunctions and enable them to manually stop the execution?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "○        If yes, which information is given to the human overseer to allow for stopping correctly the system?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "○        If yes, how are strategies adopted by the human overseer to address the system alerts traced?",
          "In-Context": ""
        },
        {
          "Compliance checklist": "",
          "Guiding Questions": "If the AI system has been developed by integrating GPAI models, what transparency measures have been adopted, taking into account the specific characteristics of such models? What technical measures and contractual safeguards have been put in place?",
          "In-Context": ""
        }
      ]
    }
  ]
}
